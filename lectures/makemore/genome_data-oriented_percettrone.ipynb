{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approccio orientato ai dati\n",
    "\n",
    "## Approccio connettivista: Perceptron\n",
    "\n",
    "L'approccio connettivista prevede l'utilizzo di un modello di rete neurale, come un semplice perceptron, per imparare i modelli dei nomi. Le fasi comprendono:\n",
    "\n",
    "1. **Codifica dei dati**: Convertire i nomi in un formato adatto all'addestramento della rete neurale (ad esempio, codifica a un punto).\n",
    "2. **Addestramento del modello**: Addestrare un semplice perceptron sul set di dati. L'input potrebbe essere costituito da sequenze di caratteri e l'obiettivo potrebbe essere il carattere successivo del nome.\n",
    "3. **Valutazione della perdita**: Utilizzare una funzione di perdita adeguata (come l'entropia incrociata categoriale) per valutare le prestazioni del modello e regolare i pesi di conseguenza.\n",
    "4. **Generazione del nome**: Utilizzare il modello addestrato per prevedere i caratteri successivi, a partire da un carattere o una sequenza iniziale, per generare nuovi nomi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = 'names.txt'\n",
    "\n",
    "# Read the file and process it for bigram frequency analysis\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    names = file.read().splitlines()\n",
    "\n",
    "print(names[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fase 1: Preparazione dei dati\n",
    "\n",
    "È necessario convertire i nomi in un formato che una rete neurale possa elaborare. Di solito questo comporta la codifica di ogni carattere in un formato numerico, come la codifica *one-hot*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a set of all unique characters in the names\n",
    "unique_chars = set(''.join(names))\n",
    "unique_chars = sorted(unique_chars)\n",
    "print(len(unique_chars), unique_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map each character to a unique integer\n",
    "char_to_int = dict((c, i) for i, c in enumerate(unique_chars))\n",
    "\n",
    "#  Create a dictionary to convert back\n",
    "int_to_char = {i: c for c, i in char_to_int.items()}\n",
    "\n",
    "# Convert names to integer representation\n",
    "int_names = [[char_to_int[char] for char in name] for name in names]\n",
    "\n",
    "# One-hot encode the integer representation of names\n",
    "max_name_length = max([len(name) for name in int_names])\n",
    "n_chars = len(unique_chars)\n",
    "one_hot_encoded = np.zeros(\n",
    "    (len(int_names), max_name_length, n_chars), dtype=np.float32)\n",
    "for i, name in enumerate(int_names):\n",
    "    for j, char_int in enumerate(name):\n",
    "        one_hot_encoded[i, j, char_int] = 1.0\n",
    "\n",
    "one_hot_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded[0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot the first two names in the dataset\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(names[0])\n",
    "plt.imshow(one_hot_encoded[0].T)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(names[1])\n",
    "plt.imshow(one_hot_encoded[1].T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 2: Progettazione del percettrone\n",
    "\n",
    "È possibile progettare un semplice perceptron utilizzando Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_size = len(char_to_int)  # Number of unique characters\n",
    "output_size = len(char_to_int)  # Same as the number of unique characters\n",
    "\n",
    "# Initialize weights and biases\n",
    "weights = np.random.normal(0.0, pow(input_size, -0.5),\n",
    "                           (input_size, output_size))\n",
    "biases = np.zeros(output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[0], biases[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fase 3: addestramento del modello\n",
    "\n",
    "L'addestramento del perceptron comporta la regolazione dei pesi in base all'errore tra l'uscita prevista e quella effettiva. Questa operazione viene tipicamente eseguita utilizzando la *backpropagation* e un algoritmo di ottimizzazione come la discesa del gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(input_layer, weights, biases):\n",
    "    return sigmoid(np.dot(input_layer, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(target, output, input_layer, weights, biases, learning_rate):\n",
    "    error = target - output\n",
    "    d_weights = np.dot(input_layer.T, (2 * error * sigmoid_derivative(output)))\n",
    "    d_biases = 2 * error * sigmoid_derivative(output)\n",
    "\n",
    "    # Calculate the updated weights and biases\n",
    "    updated_weights = weights + learning_rate * d_weights\n",
    "    updated_biases = biases + learning_rate * \\\n",
    "        np.sum(d_biases, axis=0)  # Summing up the gradients for biases\n",
    "\n",
    "    return updated_weights, updated_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel contesto dell'addestramento di una rete neurale per la generazione di nomi, il \"target\" si riferisce all'output desiderato per un dato input durante il processo di addestramento. Nel caso di un semplice perceptron per la generazione di nomi, l'obiettivo per ogni istanza di addestramento è tipicamente il carattere successivo della sequenza di nomi, codificato nello stesso formato dell'input (ad esempio, codificato a un punto, se è così che sono rappresentati i caratteri di input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming one_hot_encoded is the one-hot encoded representation of names\n",
    "# Each character in a name is an input, and the next character is the target\n",
    "\n",
    "# Split each name into input-target pairs\n",
    "inputs = []\n",
    "targets = []\n",
    "for name in one_hot_encoded:\n",
    "    for i in range(len(name) - 1):\n",
    "        inputs.append(name[i])\n",
    "        targets.append(name[i + 1])\n",
    "\n",
    "# Convert to numpy arrays for efficient computation\n",
    "inputs = np.array(inputs)\n",
    "targets = np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def generate_name(weights, biases, char_to_int, int_to_char, max_length=10):\n",
    "    # Start with a random character\n",
    "    current_char = random.choice(list(char_to_int.keys()))\n",
    "    name = current_char\n",
    "\n",
    "    for _ in range(max_length - 1):\n",
    "        # Convert current character to one-hot encoding\n",
    "        input_vec = np.zeros((1, len(char_to_int)))\n",
    "        input_vec[0, char_to_int[current_char]] = 1\n",
    "\n",
    "        # Forward pass to predict the next character\n",
    "        output = forward_pass(input_vec, weights, biases)\n",
    "\n",
    "        # Convert output to character\n",
    "        next_char_int = np.argmax(output)\n",
    "        next_char = int_to_char[next_char_int]\n",
    "\n",
    "        # Append the predicted character to the name\n",
    "        name += next_char\n",
    "\n",
    "        # Update the current character\n",
    "        current_char = next_char\n",
    "\n",
    "        # Optionally: break if a special end-of-sequence character is predicted\n",
    "\n",
    "    return name.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_perceptron(inputs, targets, weights, biases, epochs, learning_rate, char_to_int, int_to_char):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(len(inputs)):\n",
    "            input_layer = inputs[i]\n",
    "            target = targets[i]\n",
    "\n",
    "            # Forward pass\n",
    "            output = forward_pass(input_layer, weights, biases)\n",
    "\n",
    "            # Calculate loss (mean squared error)\n",
    "            loss = np.mean((target - output) ** 2)\n",
    "            total_loss += loss\n",
    "\n",
    "            # Backward pass and update weights and biases\n",
    "            weights, biases = backward_pass(\n",
    "                target, output, input_layer, weights, biases, learning_rate)\n",
    "\n",
    "        # Print epoch details and generate a name\n",
    "        average_loss = total_loss / len(inputs)\n",
    "        generated_name = generate_name(\n",
    "            weights, biases, char_to_int, int_to_char)\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{epochs}, Loss: {average_loss:.4f}, Generated Name: {generated_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "learning_rate = 0.01\n",
    "train_perceptron(inputs, targets, weights, biases, epochs,\n",
    "                 learning_rate, char_to_int, int_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggiungere un layer nascosto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(char_to_int)  # Number of unique characters\n",
    "hidden_size = 256  # You can adjust this\n",
    "output_size = len(char_to_int)  # Same as the number of unique characters\n",
    "\n",
    "# Initialize weights and biases for the input-to-hidden layer\n",
    "weights_1 = np.random.normal(\n",
    "    0.0, pow(hidden_size, -0.5), (input_size, hidden_size))\n",
    "biases_1 = np.zeros(hidden_size)\n",
    "\n",
    "# Initialize weights and biases for the hidden-to-output layer\n",
    "weights_2 = np.random.normal(\n",
    "    0.0, pow(output_size, -0.5), (hidden_size, output_size))\n",
    "biases_2 = np.zeros(output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(input_layer, weights_1, biases_1, weights_2, biases_2):\n",
    "    # Input to hidden layer\n",
    "    hidden_layer_input = np.dot(input_layer, weights_1) + biases_1\n",
    "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "    # Hidden to output layer\n",
    "    output_layer_input = np.dot(hidden_layer_output, weights_2) + biases_2\n",
    "    output = sigmoid(output_layer_input)\n",
    "\n",
    "    return output, hidden_layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(target, output, hidden_layer_output, input_layer, weights_1, biases_1, weights_2, biases_2, learning_rate):\n",
    "    # Calculate error\n",
    "    error = target - output\n",
    "\n",
    "    # Gradients for output layer\n",
    "    d_weights_2 = np.dot(hidden_layer_output.reshape(-1, 1), (2 * error * sigmoid_derivative(output)).reshape(1, -1))\n",
    "    d_biases_2 = 2 * error * sigmoid_derivative(output)\n",
    "\n",
    "    # Error for hidden layer\n",
    "    hidden_layer_error = np.dot(2 * error * sigmoid_derivative(output), weights_2.T).flatten()\n",
    "\n",
    "    # Gradients for hidden layer\n",
    "    d_weights_1 = np.dot(input_layer.reshape(-1, 1), hidden_layer_error.reshape(1, -1))\n",
    "    d_biases_1 = hidden_layer_error * sigmoid_derivative(hidden_layer_output)\n",
    "\n",
    "    # Update weights and biases\n",
    "    weights_1 += learning_rate * d_weights_1\n",
    "    biases_1 += learning_rate * np.sum(d_biases_1, axis=0)\n",
    "    weights_2 += learning_rate * d_weights_2\n",
    "    biases_2 += learning_rate * np.sum(d_biases_2, axis=0)\n",
    "\n",
    "    return weights_1, biases_1, weights_2, biases_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_name(weights_1, biases_1, weights_2, biases_2, char_to_int, int_to_char, max_length=10):\n",
    "    # Start with a random character\n",
    "    current_char = random.choice(list(char_to_int.keys()))\n",
    "    name = current_char\n",
    "\n",
    "    for _ in range(max_length - 1):\n",
    "        # Convert current character to one-hot encoding\n",
    "        input_vec = np.zeros((1, len(char_to_int)))\n",
    "        input_vec[0, char_to_int[current_char]] = 1\n",
    "\n",
    "        # Forward pass through the network (including the hidden layer)\n",
    "        hidden_layer_output = sigmoid(np.dot(input_vec, weights_1) + biases_1)\n",
    "        output = sigmoid(np.dot(hidden_layer_output, weights_2) + biases_2)\n",
    "\n",
    "        # Convert output to character\n",
    "        next_char_int = np.argmax(output)\n",
    "        next_char = int_to_char[next_char_int]\n",
    "\n",
    "        # Append the predicted character to the name\n",
    "        name += next_char\n",
    "\n",
    "        # Update the current character\n",
    "        current_char = next_char\n",
    "\n",
    "        # Optionally: break if a special end-of-sequence character is predicted\n",
    "\n",
    "    return name.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_perceptron(inputs, targets, weights_1, biases_1, weights_2, biases_2, epochs, learning_rate, char_to_int, int_to_char):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(len(inputs)):\n",
    "            input_layer = inputs[i]\n",
    "            target = targets[i]\n",
    "\n",
    "            # Forward pass\n",
    "            output, hidden_layer_output = forward_pass(\n",
    "                input_layer, weights_1, biases_1, weights_2, biases_2)\n",
    "\n",
    "            # Calculate loss (mean squared error)\n",
    "            loss = np.mean((target - output) ** 2)\n",
    "            total_loss += loss\n",
    "\n",
    "            # Backward pass and update weights and biases\n",
    "            weights_1, biases_1, weights_2, biases_2 = backward_pass(\n",
    "                target, output, hidden_layer_output, input_layer, weights_1, biases_1, weights_2, biases_2, learning_rate)\n",
    "\n",
    "        # Print epoch details and generate a name\n",
    "        average_loss = total_loss / len(inputs)\n",
    "        # Generate name using output layer weights\n",
    "        generated_name = generate_name(\n",
    "            weights_2, biases_2, char_to_int, int_to_char)\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{epochs}, Loss: {average_loss:.4f}, Generated Name: {generated_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "learning_rate = 0.01\n",
    "# Assuming that the following variables are already defined:\n",
    "# inputs, targets, weights_1, biases_1, weights_2, biases_2, epochs, learning_rate, char_to_int, int_to_char\n",
    "\n",
    "train_perceptron(inputs, targets, weights_1, biases_1, weights_2,\n",
    "                 biases_2, epochs, learning_rate, char_to_int, int_to_char)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
